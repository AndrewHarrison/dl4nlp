#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=DLNLP_BERT
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=6
#SBATCH --time=1:00:00
#SBATCH --mem=48000M
#SBATCH --output=slurm_output_%A.out

module purge
module load 2019
module load Anaconda3/2018.12

# Your job starts in the directory where you call sbatch
cd $HOME/dl4nlp/
# Activate your environment
source activate dl4nlp
# Run your code
srun python -u experiments/multi-choice/run_multiple_choice.py --data_dir data/mutual --model_type bert --model_name_or_path bert-base-uncased --task_name mutual --output_dir experiment_outputs/bert --do_lower_case --do_eval